% Presentation originally created by Malte Esders in Summer 2018
% Extended by Christopher Anders in Summer 2019
% Material from Wojciech Samek added by Malte Esders in February 2024
\documentclass[Nike]{tuberlinbeamer}
\graphicspath{{./tuberlinbeamer/}{./figures/}{./../scripts/figures}}
\usepackage[english]{babel}  % 'babel' muss geladen werden
\usepackage[utf8]{inputenc}  % optional, aber empfehlenswert
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{hyperref}
\usepackage{dsfont}
% recommended styles: authoryear, alphabetic
\usepackage[style=authoryear]{biblatex}

\bibliography{references.bib}
\setbeamertemplate{bibliography item}{}
% Use Chancery Font
\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}

\usepackage{subfig}
\usepackage{hyperref}
\usepackage{svg}
\usepackage{algorithm2e}
\usepackage{algorithmic}

\newcommand{\R}{\ensuremath{\mathds{R}}}
\newcommand{\x}{\boldsymbol{x}}
\newcommand{\y}{\boldsymbol{y}}
\newcommand{\bx}{{\mathbf x}}
\newcommand{\by}{{\mathbf y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\be}{\mathbf{e}}
\DeclareMathOperator*{\Corr}{Corr}
\DeclareMathOperator*{\E}{E}
\DeclareMathOperator*{\sign}{sign}
\DeclareMathOperator{\Cov}{Cov}


\title{Unsupervised Learning}
\author{Malte Esders, Christopher Anders, Wojciech Samek}
\institute{Technische Universität Berlin - Machine Learning Group}

% Eigenes Logo einfuegen:
\renewcommand{\pathtomylogo}{ida_logo}

\usepackage{mycommands}

\AtBeginSubsection[]
{
  \begin{frame}
    \small
    \frametitle{Agenda}
    \setcounter{tocdepth}{2}
    \tableofcontents[currentsection,currentsubsection]
  \end{frame}
}

\begin{document}
\frame{\titlepage}


\begin{frame}
  \frametitle{This presentation}
  Presentation repository: \\
  \begin{center}
    \url{https://github.com/Maltimore/unsupervised_learning_lecture}\\
  \end{center}
  Contains code to generate plots\\
  Parts not otherwise marked: BSD licensed
\end{frame}


\section{Introduction and intuition}
\subsection{Unsupervised Learning}

\begin{frame}
 \frametitle{Unsupervised Learning}
  \begin{center}
    \vfill
    \includegraphics[width=\textwidth]{ml_schema}
    \vfill
  \end{center}
\end{frame}

\begin{frame}
 \frametitle{Unsupervised Learning}
  \begin{center}
    \vfill
    \includegraphics[width=\textwidth]{ml_schema_sel}
    \vfill
  \end{center}
\end{frame}

\begin{frame}
 \frametitle{Unsupervised Learning}
  \begin{center}
    \begin{quote}
      When we’re learning to see, nobody's telling us what the right
      answers are - we just look. Every so often, your mother says
      ’that's a dog’, but that's very little information. You'd be lucky if
      you got a few bits of information - even one bit per second - that
      way. The brain's visual system has $10^{14}$ neural connections. And
      you only live for $10^9$ seconds. So it's no use learning one bit per
      second. You need more like $10^5$ bits per second. And there's only
      one place you can get that much information: from the input
      itself.
    \end{quote}
  Geoffrey Hinton, 1996
  \end{center}
  Genome: $3 * 10^9$ base-pairs (A, T, C, G), with 2 bits per location has  $6 * 10^9$ bits.
\end{frame}


\subsection{Cluster examples}
\begin{frame}
 \frametitle{Clusters - Intuition}
  \begin{center}
    Without labels:
    \centering\includegraphics[width=1.0\textwidth]{sample_clusters.pdf}
    \pause
    ``True'' clusters:
    \centering\includegraphics[width=1.0\textwidth]{sample_clusters_true_assignment.pdf}
  \end{center}
  We do not have the ``true'' clusters with real data!
\end{frame}

\begin{frame}
 \frametitle{Clusters - Intuition}
  Clustering is an ill-posed problem: no definition of a cluster.\\
  Some approaches:
  \begin{itemize}
    \item samples within a cluster should be ``similar'', samples between clusters ``different''
    \item clusters are regions of densely populated feature space
    \item samples in the same cluster come from the same underlying process
  \end{itemize}
\end{frame}


\section{Distance and Similarity measures}
\subsection{Euclidean distance}
\begin{frame}[t]
  \frametitle{Euclidean distance}
  \begin{columns}[T]
    \onslide
    \column{0.6\textwidth}
      Pythagoras formula:
      \begin{align*}
        c^2 &= a^2 + b^2\\
        c   &= \sqrt{a^2 + b^2}
      \end{align*}
      \pause
      Euclidean distance notation:
      \begin{equation*}
        d(\x, \y)= \sqrt{\Delta x_1^2 + \Delta x_2^2}
      \end{equation*}
      \only<3-5>{\
        Euclidean distance 3-D:\
        \begin{align*}\
          \overline{BC} &= \sqrt{\overline{AC}^2 + \overline{AB}^2}\\
          \only<4>{\
            \overline{BD} &= \sqrt{\overline{BC}^2 + \overline{CD}^2}\
          }\
          \only<5>{\
            \overline{BD} &= \sqrt{\overline{AC}^2 + \overline{AB}^2 + \overline{CD}^2}\
          }\
        \end{align*}\
      }
      \only<6->{\
        Euclidean distance d-D:\
        \begin{equation*}\
          d(\x, \y) = \sqrt{\sum_{i=1}^d \Delta x_i^2}\
        \end{equation*}\
      }
    \column{0.3\textwidth}
      \onslide<1->
      \includegraphics[width=.8\textwidth]{pythagoras.pdf}
      \tiny Wikipedia Pythagorean theorem
      \vskip 2mm
      \onslide<3->
      \includegraphics[width=.7\textwidth]{ndeuclidean.pdf}
  \end{columns}
\end{frame}

\begin{frame}[t]
  \frametitle{Manhattan distance}
  \begin{columns}[T]
    \column{0.6\textwidth}
      ``Manhattan'' / ``Taxicab'' distance
      \begin{equation*}
        d(\x, \y) = \sum_{i=1}^d |\Delta x_i|
      \end{equation*}
      Usage: LASSO regularization
    \column{0.3\textwidth}
      \includegraphics[width=\textwidth]{manhattan.pdf}
      \tiny Wikipedia Manhattan distance
  \end{columns}
\end{frame}

\subsection{Minkowski distance}
\begin{frame}[t]
  \frametitle{Minkowski distance}
  \begin{columns}[T]
    \column{0.6\textwidth}
      Minkowski distance generalizes Euclidean and Manhattan distance:
      \begin{equation*}
        d(\x, \y) = \Bigg(\sum_{i=1}^d |\Delta x_i|^p\Bigg)^{1/p}
      \end{equation*}
      \pause
      $p=1$: Manhattan distance\\
      $p=2$: Euclidean distance\\
      \pause
      $p=\infty$:
      \begin{equation*}
        d(\x, \y) = \max_{i} |\Delta x_i|
      \end{equation*}
      \pause
      $p\rightarrow0$:
      \begin{equation*}
        d(\x, \y) = \min_{i} |\Delta x_i|
      \end{equation*}
      \pause
    \column{0.3\textwidth}
      \includegraphics[width=0.5\textwidth]{minkowski_norms.pdf}\\
      \tiny Wikipedia Norm (mathematics)
  \end{columns}
\end{frame}

\subsection{Vector norm notation}
\begin{frame}
  \frametitle{Vector norm}
  The distance between vectors is the vector-norm of their difference:
  \begin{equation*}
    d(\x, \y) = \Bigg(\sum_{i=1}^d |\Delta x_i|^p\Bigg)^{1/p} = ||\x - \y||_p
  \end{equation*}
  \pause
  For $p=2$, the shorthand is:
  \begin{equation*}
    ||\x - \y||_2 = ||\x - \y||
  \end{equation*}
\end{frame}


\subsection{Squared euclidean distance}
\begin{frame}
  \frametitle{Squared euclidean distance}
  Squared euclidean distance:
  \begin{equation*}
    ||\x - \y||_2^2 = \sum_{i=1}^d |\Delta x_i|^2
  \end{equation*}
  \pause
  Do not confuse
  \begin{equation*}
    ||\x - \y|| = ||\x - \y||_2 \neq ||\x - \y||^2 = ||\x - \y||_2^2
  \end{equation*}
  \pause
  Squared Euclidean distance:
  \begin{itemize}
    \item not a true metric (no triangle inequality)
    \item overemphasizes big distances (and vice versa)
    \item makes some optimization problems converge better
  \end{itemize}
\end{frame}

\subsection{Cosine distance}
\begin{frame}[t]
  \frametitle{Cosine distance}
  \begin{columns}[T]
    \column{0.45\textwidth}\
    Cosine distance only compares angles between vectors.\\\
    \uncover<2->{\
      Cosine definition:\
      \begin{align*}\
        \x^\intercal \y &= |\x||\y|cos(\alpha)\\ \
        cos(\alpha) &= \frac{\x^\intercal \y}{|\x||\y|} \
      \end{align*}\
    }\
    \uncover<3->{\
      Cosine distance:\
      \begin{equation*}\
        d(\x, \y) = 1 - \frac{\x^\intercal \y}{|\x||\y|}\
      \end{equation*}\
    }\
    \column{0.45\textwidth}
    \only<1-3>{\
      \includegraphics[width=.8\textwidth]{cosine_distance}\
    }\
  \end{columns}
\end{frame}

\subsection{Z-scores}
\begin{frame}
  \frametitle{Different scales}
  Measurements can be at different scales (for instance cm, kg etc.)\\
  We can normalize:
  \begin{equation*}
    z = \frac{x - \mu}{\sigma}
  \end{equation*}
  \pause
  \centering\includegraphics[width=0.7\textwidth]{normalization_clustering.png}
  \tiny \cite{guerin_clustering_2017}
\end{frame}


\section{Clustering algorithms}
\begin{frame}
  \vfill
  \centering\includegraphics[width=\textwidth]{ml_schema_ext}
  \vfill
\end{frame}

\subsection{Hierarchical Clustering}
\begin{frame}
  \frametitle{Hierarchical Clustering}
  \begin{itemize}[<+->]
    \item graph-based (no embedding needed!)
    \item builds a hierarchy of clusters, either top down (``divisive'') or bottom up (``agglomerative'')
    \item intuitive
    \item suggests numbers of clusters
    \item arbitrary cluster shapes
    \item sensitive to noise
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Agglomerative clustering algorithm:}
  Let $\mathpzc{D}_i$ be the sets containing the samples in cluster $i$.  \\
  \bigskip

  choose $n_{final}$ clusters\\
  initialize $\mathpzc{D}_i$ as singleton clusters\\
  initialize $n_{clusters} = n_{samples}$\\
  \While{$n_{clusters} > n_{final}$}{
    find clusters $i$ and $j$ with smallest distance\\
    merge $\mathpzc{D}_i$ and $\mathpzc{D}_j$\\
    $n_{clusters} \leftarrow n_{clusters} - 1$
  }
  return clusters
\end{frame}

\begin{frame}
  \frametitle{Dendrogram with Euclidean distance}
  \begin{center}
    \centering\includegraphics[width=0.8\textwidth]{dendrogram_euclidean.pdf}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Dendrogram with squared Euclidean distance}
  \begin{center}
    \centering\includegraphics[width=0.8\textwidth]{dendrogram_squared_euclidean.pdf}
  \end{center}
\end{frame}


\begin{frame}
  \frametitle{Cluster distance measures}
  Let $\mathpzc{D}_i$, $\mathpzc{D}_j$ be the sets containing the samples in clusters $i$ and $j$.
  \\
  Minimum linkage:
  \begin{equation*}
    d_{min}(\mathpzc{D}_i, \mathpzc{D}_j) = \min_{\x \in \mathpzc{D}_i, \x' \in \mathpzc{D}_j} ||\x - \x'||
  \end{equation*}
  \pause
  \\
  Maximum or ``complete'' linkage:
  \begin{equation*}
    d_{max}(\mathpzc{D}_i, \mathpzc{D}_j) = \max_{\x \in \mathpzc{D}_i, \x' \in \mathpzc{D}_j} ||\x - \x'||
  \end{equation*}
  \\
  \pause
  Average linkage:
  \begin{equation*}
    d_{avg}(\mathpzc{D}_i, \mathpzc{D}_j) = \frac{1}{n_i n_j} \sum_{\x \in \mathpzc{D}_i} \sum_{\x' \in \mathpzc{D}_j} ||\x - \x'||
  \end{equation*}
  \scriptsize Notation from \cite{duda2012pattern}
\end{frame}

\begin{frame}
  \frametitle{Original data}
  After clustering, assignments can seem ``natural'', good to compare original data:
  \begin{center}
    \centering\includegraphics[width=0.95\textwidth]{sample_clusters.pdf}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Agglomerative clustering results}
  \begin{center}
    \scriptsize Minimum
    \centering\includegraphics[width=0.95\textwidth]{agglomerative_clusters_minimum_linkage.pdf}
    \pause
    \scriptsize Maximum
    \centering\includegraphics[width=0.95\textwidth]{agglomerative_clusters_complete_linkage.pdf}
    \pause
    \scriptsize Average
    \centering\includegraphics[width=0.95\textwidth]{agglomerative_clusters_average_linkage.pdf}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Tips and tricks}
  \begin{center}
   \centering\includegraphics[width=0.90\textwidth]{sklearn_API_doc.png}
  \end{center}
  Sometimes, documentation can be confusing:
  \\
  What is the difference between ``l2'' and ``euclidean''?
  \\
  Or ``l1'' and ``manhattan''?
\end{frame}


\begin{frame}
  \frametitle{Tips and tricks}
  Looking into the code:
  \begin{center}
    \centering\includegraphics[width=0.95\textwidth]{sklearn_code.png}
  \end{center}
  In the background it does exactly the same.
\end{frame}

\subsection{K-Means}
\begin{frame}
  \frametitle{K-Means}
  Idea: prototype for every cluster
  \\
  Prototype is mean of all samples in cluster $c$, $\mathpzc{D}_c$:
  \begin{equation*}
    \mu_c = \frac{1}{n_c} \sum_{i \in \mathpzc{D}_c} x_i
  \end{equation*}
  \pause
  Minimize distance from prototype to each member in the cluster:
  \begin{equation*}
    \mathpzc{L} = \sum_c \sum_{i \in \mathpzc{D}_c} d(\x_i, \mu_c)
  \end{equation*}
  \pause
  Finding a solution is NP-hard
  \\
  K-means cannot work with pairwise data such as user ratings
\end{frame}


\begin{frame}
  \frametitle{K-Means algorithm}
  initialize $\mu_c$ to random samples\\
  \While{clusters change}{
    assign $\mu_c$\ to cluster means\\
    assign samples to clusters\\
  }
  return clusters
\end{frame}


\begin{frame}
  \frametitle{K-Means clusters}
  \begin{center}
    K-Means assignment:
    \centering\includegraphics[width=1.0\textwidth]{kmeans_clusters.pdf}
    \pause
    Voronoi tesselation:
    \centering\includegraphics[width=1.0\textwidth]{kmeans_clusters_voronoi.pdf}
  \end{center}
  Voronoi diagram not possible with hierarchical clustering!
\end{frame}

\subsection{DBSCAN}
\begin{frame}
  \frametitle{DBSCAN}
  \begin{itemize}
    \item clusters are regions of high density separated by regions of low density
    \item arbitrary cluster shapes
  \end{itemize}
  \begin{centering}
  \includegraphics[width=0.5\textwidth]{DBSCAN_data.png}
  \tiny \cite{ester1996density}\\
  \end{centering}
  \pause
  \normalsize
  \begin{itemize}
    \item very simple, but effective (2014 SIGKDD test-of-time award (\cite{schubert2017dbscan}))\\
    \item explicit noise point modeling
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{DBSCAN}
  \begin{columns}[T]
    \column{0.6\textwidth}
      \onslide<1->
       Two parameters:
       \begin{itemize}
         \item $\epsilon$: distance, defines a neighborhood
         \item MinPts: amount of neighbors to be a core point
       \end{itemize}
       \pause
      Three types of points:
      \begin{itemize}
        \item core: has MinPts points in its epsilon-neighborhood
        \item border: within epsilon-neighborhood of core point
        \item noise: all other points
      \end{itemize}
    \column{0.3\textwidth}
      \onslide<3->
      \includegraphics[width=1.0\textwidth]{DBSCAN_illustration.png}\\
      \tiny (Wikipedia "DBSCAN")
  \end{columns}
\end{frame}


\begin{frame}
  \frametitle{DBSCAN clustering results}
  \begin{center}
    \scriptsize $\epsilon = 0.1$, MinPts $=4$
    \centering\includegraphics[width=0.95\textwidth]{{DBSCAN_clusters_eps_0.1_MinPts_4}.pdf}
    \pause
    \scriptsize $\epsilon = 0.2$, MinPts $=4$
    \centering\includegraphics[width=0.95\textwidth]{{DBSCAN_clusters_eps_0.2_MinPts_4}.pdf}
    \pause
    \scriptsize $\epsilon = 0.2$, MinPts $=20$
    \centering\includegraphics[width=0.95\textwidth]{{DBSCAN_clusters_eps_0.2_MinPts_20}.pdf}
  \end{center}
\end{frame}


\subsection{More examples}
\begin{frame}
  \frametitle{Overview of clustering algorithms}
  \begin{center}
    \includegraphics[width=0.85\textwidth]{sklearn_doc_examples.png}
  \end{center}
  \tiny \url{http://scikit-learn.org/stable/modules/clustering.html} \cite{pedregosa2011scikit}
\end{frame}


\section{Dimensionality reduction}
\subsection{Principal Component Analysis}
%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Dimensionalty Reduction - Motivation}
In many applications, we have
\begin{itemize}
\item high-dimensional data
\item reason to believe they lie close to a lower dimensional subspace
\item[$\rightarrow$] Fewer parameters needed to account for the data properties \\
{\em hidden causes} or {\em latent variables}
\end{itemize}
\vspace{1em} \pause
Examples:
\begin{itemize}
\item you want to classify high resolution images
\item you want to make a predictive model based on hundreds of customer attributes
\item you want to analyse high dimensional neural data
\item you want to detect trends in news data
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Why Dimensionalty Reduction}
\begin{itemize}
\item \textbf{Visualization}:  \\ Insights into high-dimensional structures in the data
\item \textbf{Better Generalization}: \\ Fewer dimensions $\rightarrow$ less chances of overfitting
\item \textbf{Speeding up} learning algorithms: \\
Most algorithms scale badly with increasing data dimensionality
\item \textbf{Data compression}: \\
Less storage requirements
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{The mathematical model for dimensionalty reduction}
We have
\begin{itemize}
\item high-dimensional data $X=[\bx_1,\bx_2,\dots,\bx_N]\in\R^{D\times N}$
\item reason to believe they lie close to a lower dimensional subspace
\item[$\rightarrow$] $k < D$ parameters needed to account for the data properties \\
{\em hidden causes} or {\em latent variables}  $H \in \R^{k \times N}$
\end{itemize}
\vspace{1em} \pause
\textbf{Goal:}  Find $k < D$  hidden causes  $H \in \R^{k \times N}$ , that explain the observed data via a mixing $W \in \R^{D \times k}$:
\[ X \approx W H\]
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Principal Component Analysis}
\small
We obtained some data $X=[\bx_1,\bx_2,\dots,\bx_N]\in\R^{D\times N}$\\
PCA finds a direction $\bw \in\R^D$ such that the variance of the projected data $\bw^\top X$
is maximal  \pause
\begin{eqnarray*}
\Var(\bw^T X) &=& \frac{1}{N} \sum_{n = 1}^{N} (\bw^\top \bx_n - \E(\bw^\top \bx))^2 \\ \pause
 &=& \frac{1}{N} \sum_{n = 1}^{N} (\bw^\top \bx_n - \bw^\top \E(\bx))^2 = \frac{1}{N} \sum_{n = 1}^{N} (\bw^\top (\bx_n - \E(\bx)))^2 \\\pause
 &=& \frac{1}{N} \sum_{n = 1}^{N} \bw^\top (\bx_n - \E(\bx)) \cdot  (\bx_n - \E(\bx))^\top \bw \\\pause
 &=& \bw^\top \underbrace{\left( \frac{1}{N} \sum_{n = 1}^{N}  (\bx_n - \E(\bx)) \cdot  (\bx_n - \E(\bx))^\top \right)}_{\text{Covariance matrix S}}\bw
\end{eqnarray*}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Principal Component Analysis}
\small
We obtained some data $X=[\bx_1,\bx_2,\dots,\bx_N]\in\R^{D\times N}$\\
\vspace{1em}
PCA finds a direction $\bw \in\R^D$ such that the variance of the projected data $\bw^\top X$
is maximal
\begin{eqnarray*}
\Var(\bw^T X) &=&  \bw^\top \underbrace{\left( \frac{1}{N} \sum_{n = 1}^{N}  (\bx_n - \E(\bx)) \cdot  (\bx_n - \E(\bx))^\top \right)}_{\text{Covariance matrix S}}\bw \\ \pause
 &=&  \bw^\top X X^\top\bw
\end{eqnarray*}
\hfill where we assume centered data\\
\vspace{1em} \pause
And: we need to constrain $\bw$
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Principal Component Analysis}
PCA finds a direction $\bw \in\R^D$ such that the variance of the projected data $\bw^T X$
is maximal
\begin{align} \nonumber
 \argmax_w \frac{\bw^{\top} S\bw}{\bw^\top \bw}
\end{align}

This objective function is independent of the scaling of $\bw$.  \\ \pause
\vspace{2em}
Note the similarity to the objective of Linear Discriminant Analysis! \\
\vspace{1em}
$\rightarrow$ Different covariance matrices, different problem, but: same maths solve it
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Principal Component Analysis}
\begin{align}\nonumber
\argmax_{\bw} &\frac{\bw^{\top} S \bw}{\bw^\top\bw}
\end{align}
Set the derivative w.r.t $\bw$ to zero:\\
\begin{align}\nonumber
\frac{(\bw^{\top}\bw) 2 S \bw - (\bw^{\top}S \bw) 2 \bw}{(\bw^{\top}\bw)^2}= &0\\ \nonumber
(\bw^{\top}S\bw)\bw = & (\bw^{\top}\bw)S\bw\\ \nonumber
\bw = &S \bw \cdot \underbrace{\frac{\bw^{\top}\bw}{\bw^{\top}S\bw}}_{scalar} \\ \nonumber
S \bw = & \lambda \bw
\end{align}
This is the standard eigenvalue problem.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Principal Component Analysis}
For $S \bw = \lambda \bw $, we see that the variance in direction $\bw$ is given by:
\begin{align}\nonumber
\argmax_{\bw} &\frac{\bw^{\top} S \bw}{\bw^\top\bw} =  \frac{\bw^{\top} \lambda \bw}{\bw^\top\bw} = \lambda
\end{align}
The variance of the projected data in an eigendirection $\bw$ is given by the corresponding eigenvalue! \\ \pause
\vspace{2em}
The direction of maximal variance in the data is equal to the eigenvector having the largest eigenvalue.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Principal Component Analysis}
\scriptsize
\begin{algorithm}[H]
  \caption{Principal Component Analysis}
  \begin{algorithmic}[1]
    \REQUIRE data $x_1, \ldots, x_N \in \R^d$, number of principal components $k$
    \STATE \# Center Data
     \STATE $X = X - 1/N\sum_ix_i$
     \STATE \# Compute Covariance Matrix
     \STATE $C = 1/N ~XX^{\top}$
     \STATE \# Compute eigenvectors corresponding to the $k$ largest eigenvalues
     \STATE $W = \text{eig}(C)$
     \STATE \# Project data onto $W$
     \STATE $H = W^\top X$
\RETURN W, H
  \end{algorithmic}
\end{algorithm}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Principal Component Analysis}
\begin{columns}
\column{.5\textwidth}
\centering
Before PCA
\includegraphics[width=.8\textwidth]{figures/PCA_toy_before}

\column{.5\textwidth}
\pause
\centering
After PCA
\includegraphics[width=.8\textwidth]{figures/PCA_toy_after}
\end{columns}
\centering

\pause
PCA aligns maximum variance directions with standard basis\\
$\rightarrow$ Variance along each dimension is {\bf uncorrelated}\\
$\rightarrow$ Now we can remove each dimension separately\\
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Non-Negative Matrix Factorization}
\begin{frame}\frametitle{Non-Negative Matrix Factorization}
\begin{itemize}
\item For some data PCA does not make sense
\item Example: Non-negative data
\begin{itemize}
\item Principal directions will have negative entries
\item This can be hard to interpret
\end{itemize}
\item Many data sets are strictly positive
\begin{itemize}
\item Text data
\item Image data
\item Probabilistic data
\end{itemize}
\item Very easy to implement
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Non-Negative Matrix Factorization}
\centering
Given non-negative data $X\in\R^{D\times N}_+$ we want to find $W\in\R_+^{D\times C},~H\in\R_+^{C\times N}$ such that\\
\begin{align} \nonumber
\argmin_{W,H}||X-WH||_{\text{Fro}}^{2} = \argmin_{W,H} \sum_{d = 1}^{D}  \sum_{n = 1}^{N} (X_{dn}- (WH)_{dn})^{2}  \\\nonumber
\end{align}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Non-Negative Matrix Factorization}
\begin{itemize}
\item Local minima  \pause
\item Gradient descent finds an optimal solution by iterating
\begin{align}\nonumber
H \leftarrow H + \eta \left(W^{\top}WH - X^{\top}W\right)\\ \nonumber
W \leftarrow W + \eta \left(WHH^{\top} - XH^{\top}\right) \nonumber
\end{align} \pause
\item By choosing $\eta$ wisely one can transform the additive updates into multiplicative ones:
\begin{align}\nonumber
H &= H \odot W^{\top}X \oslash W^{\top}WH\\ \nonumber
W  & = W \odot XH^{\top} \oslash WHH^{\top}
\end{align}
where
\begin{itemize}
\item[$\odot$] is {\em element-wise} multiplication (in python '*')
\item[$\oslash$] is {\em element-wise} division (in python '/')
\end{itemize}
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{NMF Algorithm}
\scriptsize
\begin{algorithm}[H]
  \caption{Non-negative Matrix Factorization}
  \begin{algorithmic}[1]
    \REQUIRE data $X=[x_1, \ldots, x_N] \in \R_+^{D\times N}$, number of factors $k$
    \STATE \# Initialize $W \in \R_+^{D\times k},~H \in \R_+^{k\times N}$ randomly
    \STATE \# Add a small constant $\epsilon =10^{-19}$ to $X$ to avoid zero-divisions
     \FOR{ it $\le$ Iterations}
     \STATE $H = H \odot W^{\top}X \oslash W^{\top}WH$
     \STATE $W = W \odot XH^{\top} \oslash WHH^{\top}$
     \ENDFOR
\RETURN $W,H$
  \end{algorithmic}
\end{algorithm}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{NMF Applications - Face Parts}
\centering
\includegraphics[width=.5\textwidth]{figures/nmf_imag2}\\
\includegraphics[width=.5\textwidth]{figures/nmf_imag}\\
\end{frame}

\begin{frame}%[allowframebreaks]  % Uncomment if you need more than one page for references
  \frametitle{References}
  \printbibliography
\end{frame}

\end{document}
