@article{hinton_reducing_2006,
	title = {Reducing the {Dimensionality} of {Data} with {Neural} {Networks}},
	volume = {313},
	copyright = {American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/313/5786/504},
	doi = {10.1126/science.1127647},
	abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.
Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks.
Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks.},
	language = {en},
	number = {5786},
	urldate = {2018-07-20},
	journal = {Science},
	author = {Hinton, G. E. and Salakhutdinov, R. R.},
	month = jul,
	year = {2006},
	pmid = {16873662},
	pages = {504--507},
	file = {Hinton_Salakhutdinov_2006_Reducing the Dimensionality of Data with Neural Networks.pdf:/home/maltimore/tubCloud/Studium/zotero_articles/Explaining/Hinton_Salakhutdinov_2006_Reducing the Dimensionality of Data with Neural Networks.pdf:application/pdf;Snapshot:/home/maltimore/tubCloud/Studium/Zotero/storage/TJT2T6BU/tab-pdf.html:text/html}
}


@article{guerin_clustering_2017,
	title = {Clustering for {Different} {Scales} of {Measurement} - the {Gap}-{Ratio} {Weighted} {K}-means {Algorithm}},
	url = {http://arxiv.org/abs/1703.07625},
	abstract = {This paper describes a method for clustering data that are spread out over large regions and which dimensions are on diﬀerent scales of measurement. Such an algorithm was developed to implement a robotics application consisting in sorting and storing objects in an unsupervised way. The toy dataset used to validate such application consists of Lego bricks of diﬀerent shapes and colors. The uncontrolled lighting conditions together with the use of RGB color features, respectively involve data with a large spread and diﬀerent levels of measurement between data dimensions. To overcome the combination of these two characteristics in the data, we have developed a new weighted K-means algorithm, called gap-ratio K-means, which consists in weighting each dimension of the feature space before running the K-means algorithm. The weight associated with a feature is proportional to the ratio of the biggest gap between two consecutive data points, and the average of all the other gaps. This method is compared with two other variants of K-means on the Lego bricks clustering problem as well as two other common classiﬁcation datasets.},
	language = {en},
	urldate = {2018-08-29},
	journal = {arXiv:1703.07625 [cs, stat]},
	author = {Guérin, Joris and Gibaru, Olivier and Thiery, Stéphane and Nyiri, Eric},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.07625},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Data Structures and Algorithms},
	annote = {Comment: 13 pages, 6 figures, 2 tables. This paper is under the review process for AIAP 2017},
	file = {Guérin et al. - 2017 - Clustering for Different Scales of Measurement - t.pdf:/home/maltimore/tubCloud/Studium/Zotero/storage/NNMZSHLB/Guérin et al. - 2017 - Clustering for Different Scales of Measurement - t.pdf:application/pdf}
}

@article{pedregosa2011scikit,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={Journal of machine learning research},
  volume={12},
  number={Oct},
  pages={2825--2830},
  year={2011}
}

@book{duda2012pattern,
  title={Pattern classification},
  author={Duda, Richard O and Hart, Peter E and Stork, David G},
  year={2012},
  publisher={John Wiley \& Sons}
}

@inproceedings{ester1996density,
  title={A density-based algorithm for discovering clusters in large spatial databases with noise.},
  author={Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"o}rg and Xu, Xiaowei and others},
  booktitle={Kdd},
  volume={96},
  number={34},
  pages={226--231},
  year={1996}
}


@article{schubert2017dbscan,
  title={DBSCAN revisited, revisited: why and how you should (still) use DBSCAN},
  author={Schubert, Erich and Sander, J{\"o}rg and Ester, Martin and Kriegel, Hans Peter and Xu, Xiaowei},
  journal={ACM Transactions on Database Systems (TODS)},
  volume={42},
  number={3},
  pages={19},
  year={2017},
  publisher={ACM}
}

